{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Ex\n",
    "- https://wikidocs.net/22660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일을 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/youngheelee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize를 하기 위해서 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('data/ted_en-20160408.xml', 'r', encoding='UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = etree.parse(targetXML)\n",
    "# XML 파일로부터 <content>와 </content> 사이의 내용만 가져옴\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거합니다.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거하는 코드입니다.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 코퍼스에 대해서 nltk를 이용하여 문장 토큰화를 수행합니다.\n",
    "sent_text = sent_tokenize(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    # 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환합니다.\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
     ]
    }
   ],
   "source": [
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC\n",
    "- **Word2Vec의 하이퍼파라미터값**\n",
    "- sentences = 학습데이터\n",
    "- size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "- window = 컨텍스트 윈도우 크기\n",
    "- min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "- workers = 학습을 위한 프로세스 수\n",
    "- sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = result, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teacher', 0.8017913103103638),\n",
       " ('friend', 0.7812360525131226),\n",
       " ('parent', 0.7393051385879517),\n",
       " ('physician', 0.737319827079773),\n",
       " ('son', 0.7372961640357971),\n",
       " ('dog', 0.7358255386352539),\n",
       " ('husband', 0.7327225804328918),\n",
       " ('mom', 0.7192235589027405),\n",
       " ('dad', 0.7103975415229797),\n",
       " ('kid', 0.7099260091781616)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주어진 단어와 유사한 단어들을 얻는다\n",
    "#a = model.most_similar(\"cancer\")\n",
    "a = model.wv.most_similar(\"doctor\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distraction', 0.8094371557235718),\n",
       " ('organizational', 0.7916973829269409),\n",
       " ('currency', 0.7802987098693848),\n",
       " ('hormone', 0.7762454748153687),\n",
       " ('replication', 0.7682510614395142),\n",
       " ('transistor', 0.7663240432739258),\n",
       " ('instability', 0.7647833824157715),\n",
       " ('coherent', 0.7609773874282837),\n",
       " ('autonomy', 0.7605769038200378),\n",
       " ('mutation', 0.7603967189788818)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.wv.most_similar(\"measurement\")\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 0.6623709201812744),\n",
       " ('they', 0.5871874094009399),\n",
       " ('yourself', 0.5491530895233154),\n",
       " ('i', 0.5472237467765808),\n",
       " ('someone', 0.4298765957355499),\n",
       " ('them', 0.4189791977405548),\n",
       " ('kids', 0.4152143597602844),\n",
       " ('anybody', 0.4098857641220093),\n",
       " ('somebody', 0.4079456031322479),\n",
       " ('people', 0.4036228358745575)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.wv.most_similar(\"you\")\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3748521   0.5062695   0.50360435  2.6559136  -0.42858556  1.3411982\n",
      " -0.0568492  -0.9350439  -0.24525303  2.556723    0.0412237  -0.23402838\n",
      "  3.5042133   1.0905896  -2.6384213   0.08368114  1.8363135   1.2576654\n",
      "  1.3857656   0.5277169   1.3629739   1.8929266   0.43740308 -0.3494767\n",
      "  1.9480904   1.5893017   1.4499472   0.03141216  0.6744299  -1.216567\n",
      "  1.6102592  -2.9059722  -0.8996398   0.70955724  0.95587814  2.601685\n",
      "  1.9102682  -0.5122922  -3.0147746  -0.6764171   0.99156654  0.02007678\n",
      " -2.7924004   0.18435444  0.4255929   2.5811422  -0.9718827  -0.32065687\n",
      "  1.1727475  -1.1242533   0.0558906   0.8667571   3.5646484  -1.5204134\n",
      " -0.6668468   1.00777     1.5052149   0.14583844  1.8459806  -1.1920087\n",
      " -1.3360915  -0.40612     0.5580073   1.9999878  -1.9702897   1.7534958\n",
      " -0.9678725  -1.7395809  -2.429156    2.0007944  -0.33248404 -2.520368\n",
      "  1.0651721   0.55678105  0.2519384  -1.592892   -3.031087   -0.10022679\n",
      "  2.0138917   0.9806277  -1.6577663   1.0535747   1.14044    -0.3776498\n",
      " -2.655289   -0.5307004   0.03852867 -1.1863034   1.742003   -0.37275508\n",
      " -0.28138697  0.3843509   1.7200836  -0.1235631   0.01093523  2.7354467\n",
      "  2.2586403   0.63482195  0.5833485  -0.33748305]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['you'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미 훈련된 Word2vec 임베딩(Pre-trained Word2vec embedding) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 영어\n",
    "- 구글이 제공하는 미리 학습된 Word2vec 모델을 사용하는 방법에 대해서 알아보도록 하겠습니다. \n",
    "- 구글은 이미 훈련된 Word2vec 모델의 약 3백만 개의 단어 벡터들을 제공합니다. \n",
    "- 각 벡터의 크기는 300입니다. \n",
    "- gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. \n",
    "- 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다.\n",
    "- 모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "- 압축 파일의 용량은 약 1.5GB이지만, 파일의 압축을 풀면 약 3.3GB 정도의 파일이 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape\n",
    "# (3000000, 300)\n",
    "# 3 million words * 300 features * 4bytes/feature = ~3.35GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40797036387785435\n",
      "0.057204389197708934\n",
      "[('coffees', 0.721267819404602), ('gourmet_coffee', 0.7057086825370789), ('Coffee', 0.6900454759597778), ('o_joe', 0.6891065835952759), ('Starbucks_coffee', 0.6874972581863403), ('coffee_beans', 0.6749704480171204), ('latté', 0.664122462272644), ('cappuccino', 0.662549614906311), ('brewed_coffee', 0.6621608734130859), ('espresso', 0.6616826057434082)]\n",
      "0.17583632099414304\n",
      "0.5444119091799062\n",
      "0.47714654709000137\n",
      "[('physician', 0.780602216720581), ('doctors', 0.747657299041748), ('gynecologist', 0.6947519183158875), ('surgeon', 0.6793397665023804), ('dentist', 0.6785441637039185), ('pediatrician', 0.664313793182373), ('pharmacist', 0.653485894203186), ('neurologist', 0.6517742872238159), ('cardiologist', 0.6352297067642212), ('nurse', 0.6319523453712463)]\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('this', 'is'))\n",
    "print(model.similarity('post', 'book'))\n",
    "print(model.most_similar('coffee'))\n",
    "print(model.similarity('coffe', 'morning'))\n",
    "print(model.similarity('coffe', 'starbucks'))\n",
    "print(model.similarity('coffe', 'espresso'))\n",
    "print(model.most_similar('doctor'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('afternoon', 0.8849616646766663), ('evening', 0.7173714637756348), ('moring', 0.6748378276824951), ('midday', 0.6500248312950134), ('night', 0.6452910304069519), ('afteroon', 0.6442490816116333), ('Friday', 0.640130341053009), ('Monday', 0.6309289336204529), ('midmorning', 0.6087148189544678), ('day', 0.6069665551185608)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('morning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 한국어\n",
    "- 한국어의 미리 학습된 Word2Vec 모델은 박규병님의 깃허브 주소인 https://github.com/Kyubyong/wordvectors 에 공개가 되어져 있습니다. \n",
    "- 박규병님이 공개한 직접적인 다운로드 링크는 아래와 같습니다.\n",
    "- 모델 다운로드 경로 : https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view\n",
    "- 위의 링크로부터 77MB 크기의 ko.zip 파일을 다운로드받아, 압축을 풀면 ko.bin이라는 50MB 크기의 파일이 있습니다. \n",
    "- 이 파일을 로드하고, 유사도를 계산해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('../model/ko/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5022125819449108\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('아침', '출근'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('저녁', 0.8432559967041016), ('밤', 0.7393144369125366), ('새벽', 0.7126438617706299), ('금요일', 0.6616203784942627), ('토요일', 0.6555814743041992), ('주말', 0.6473068594932556), ('그날', 0.6318601965904236), ('점심', 0.6233175992965698), ('정오', 0.6203576326370239), ('평일', 0.6187670826911926)]\n"
     ]
    }
   ],
   "source": [
    "a=model.wv.most_similar('아침')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <참고>\n",
    "- Word2vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이지만 \n",
    "- 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델입니다. \n",
    "- 우선 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어입니다.\n",
    "\n",
    "- 링크 : https://brunch.co.kr/@goodvc78/16?fbclid=IwAR1QZZAeZe_tNWxnxVCRwl8PIouBPAaqSIJ1lBxJ-EKtfDfmLehi1MUV_Lk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from nltk import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia ( (listen),  (listen) WIK-ih-PEE-dee-ə) is a multilingual online encyclopedia with exclusively free content and no ads, based on open collaboration through a model of content edit by web-based applications like web browsers, called wiki. It is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites by Alexa rank as of April 2019. It is owned and supported by the Wikimedia Foundation, a non-profit organization that operates on money it receives from donors to remain ad free.Wikipedia was launched on January 15, 2001, by Jimmy Wales and Larry Sanger. Sanger coined its name, as a portmanteau of wiki (the Hawai'ian word for \"quick\") and \"encyclopedia\". Initially an English-language encyclopedia, versions in other languages were quickly developed. With 5,846,932 articles, the English Wikipedia is the largest of the more than 290 Wikipedia encyclopedias. Overall, Wikipedia comprises more than 40 million articles in 301 different languages and by February 2014 it had reached 18 billion page views and nearly 500 million unique visitors per month.In 2005, Nature published a peer review comparing 42 hard science articles from Encyclopædia Britannica and Wikipedia and found that Wikipedia's level of accuracy approached that of Britannica, although critics suggested that it might not have fared so well in a similar study of a random sampling of all articles or one focused on social science or contentious social issues. The following year, Time magazine stated that the open-door policy of allowing anyone to edit had made Wikipedia the biggest and possibly the best encyclopedia in the world, and was a testament to the vision of Jimmy Wales.Wikipedia has been criticized for exhibiting systemic bias, for presenting a mixture of \"truths, half truths, and some falsehoods\", and for being subject to manipulation and spin in controversial topics. In 2017, Facebook announced that it would help readers detect fake news by suggesting links to related Wikipedia articles. YouTube announced a similar plan in 2018.\n"
     ]
    }
   ],
   "source": [
    "print( wikipedia.summary(\"Wikipedia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "page = wikipedia.page('deep learning')\n",
    "sentences = [word_tokenize(sent) for sent in sent_tokenize(page.content)]\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page = wikipedia.page('machine learning')\n",
    "sentences = [word_tokenize(sent) for sent in sent_tokenize(page.content)]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep',\n",
       " 'learning',\n",
       " '(',\n",
       " 'also',\n",
       " 'as',\n",
       " 'deep',\n",
       " 'or',\n",
       " ')',\n",
       " 'is',\n",
       " 'of',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'methods',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'layers',\n",
       " 'used',\n",
       " 'in',\n",
       " 'artificial',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'Learning',\n",
       " 'can',\n",
       " 'be',\n",
       " 'supervised',\n",
       " ',',\n",
       " 'architectures',\n",
       " 'such',\n",
       " 'belief',\n",
       " 'and',\n",
       " 'have',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'including',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'network',\n",
       " 'translation',\n",
       " 'medical',\n",
       " 'image',\n",
       " 'analysis',\n",
       " 'where',\n",
       " 'they',\n",
       " 'produced',\n",
       " 'results',\n",
       " 'human',\n",
       " 'Neural',\n",
       " 'were',\n",
       " 'by',\n",
       " 'information',\n",
       " 'biological',\n",
       " 'systems',\n",
       " 'from',\n",
       " 'which',\n",
       " 'them',\n",
       " 'with',\n",
       " 'brain',\n",
       " 'most',\n",
       " '==',\n",
       " 'algorithms',\n",
       " 'that',\n",
       " 'use',\n",
       " 'multiple',\n",
       " 'units',\n",
       " 'for',\n",
       " 'feature',\n",
       " 'Each',\n",
       " 'layer',\n",
       " 'uses',\n",
       " 'output',\n",
       " 'input',\n",
       " 'learn',\n",
       " 'e.g.',\n",
       " 'classification',\n",
       " 'unsupervised',\n",
       " 'levels',\n",
       " 'representations',\n",
       " 'different',\n",
       " ';',\n",
       " 'models',\n",
       " 'are',\n",
       " 'an',\n",
       " 'generative',\n",
       " 'each',\n",
       " 'its',\n",
       " 'data',\n",
       " 'into',\n",
       " 'more',\n",
       " 'In',\n",
       " 'may',\n",
       " 'first',\n",
       " 'recognize',\n",
       " 'process']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#model = Word2Vec(sentences, min_count=4, size=50, window=10, workers=4)\n",
    "model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "list(model.wv.vocab.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 0.999549925327301), ('of', 0.999518632888794), (',', 0.9995176792144775), (')', 0.9995075464248657), ('to', 0.9995005130767822), ('the', 0.9994872212409973), ('learning', 0.999469518661499), ('in', 0.9994693994522095), ('as', 0.999406099319458), ('on', 0.9993774890899658)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994695525789743\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('deep', 'learning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
