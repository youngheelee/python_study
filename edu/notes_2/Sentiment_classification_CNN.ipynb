{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification\n",
    "- How to Develop a Word Embedding Model for Predicting Movie Review Sentiment\n",
    "- http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "- https://github.com/bhaveshoswal/CNN-text-classification-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Data\n",
    "- Movie review data from Rotten Tomatoes (http://www.cs.cornell.edu/people/pabo/movie-review-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_helpers import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, vocabulary, vocabulary_inv = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10662, 56)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16683, 14037,  9038,  4652, 16900,  1764, 16683,   395,  2884,\n",
       "         206, 11215,  3564,  1059, 16680,  7832,   206,  7341, 16900,\n",
       "       10171,   480, 15586,  5955,  7487, 16673,  1295, 14445,   292,\n",
       "        9139,  3199, 17848,  4229, 11610, 15831, 14595,   473,   473,\n",
       "         473,   473,   473,   473,   473,   473,   473,   473,   473,\n",
       "         473,   473,   473,   473,   473,   473,   473,   473,   473,\n",
       "         473,   473])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'mike',\n",
       " 'is',\n",
       " 'a',\n",
       " 'slight',\n",
       " 'and',\n",
       " 'uninventive',\n",
       " 'movie',\n",
       " 'like',\n",
       " 'the',\n",
       " 'exalted',\n",
       " 'michael',\n",
       " 'jordan',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'in',\n",
       " 'the',\n",
       " 'title',\n",
       " ',',\n",
       " 'many',\n",
       " 'can',\n",
       " 'aspire',\n",
       " 'but',\n",
       " 'none',\n",
       " 'can',\n",
       " 'equal',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocabulary_inv[i] for i in x[10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD/>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_inv[473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "\n",
    "# x.shape -> (10662, 56)\n",
    "# y.shape -> (10662, 2)\n",
    "# len(vocabulary) -> 18765\n",
    "# len(vocabulary_inv) -> 18765\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train.shape -> (8529, 56)\n",
    "# y_train.shape -> (8529, 2)\n",
    "# X_test.shape -> (2133, 56)\n",
    "# y_test.shape -> (2133, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1] # 56\n",
    "vocabulary_size = len(vocabulary_inv) # 18765\n",
    "embedding_dim = 64 # cpu에서 돌리기 위해서 조정 : org : 128\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 32 # <-64\n",
    "drop = 0.5\n",
    "\n",
    "# epochs는 현재는 많이 중요하지 않다. 많은 숫자로 설정하고 체크포인트를 보면서 조정\n",
    "epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design\n",
    "Keras에는 두 가지의 모델 생성 방법이 있습니다.\n",
    "\n",
    "1. Sequential Models\n",
    "2. Functional Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential model API**는 상당히 쉽게 딥러닝 모델을 생성하는 인터페이스를 제공하지만 한 방향성으로만 모델을 생성시킨다는 단점이 있습니다. 따라서 다음의 경우에는 Sequential model API로 모델을 생성하기가 어렵습니다.\n",
    "\n",
    "1. 다중의 입력 소스를 만들 경우\n",
    "2. 다중의 출력 층을 만들 경우\n",
    "3. 층을 여러 방향으로 공유하는 경우 등."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 **Functional model API**를 이용하는 것입니다. 이 방법은 좀 더 유연하게 딥러닝 모델을 디자인할 수 있게 합니다.\n",
    "만드는 것은 전혀 어렵지 않습니다. `keras.models.Model`을 활용하여 생성할 수 있으며 **Input**과 **Output**만 잘 정의해주면 됩니다.\n",
    "\n",
    "**Functional model API**에 대한 자세한 가이드는 Keras 공식 문서 (https://keras.io/getting-started/functional-api-guide/)를 참고하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 **Functional model API**로 모델을 생성한 경우입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(?, 56), dtype=int32)\n",
      "Tensor(\"embedding_3/embedding_lookup/Identity:0\", shape=(?, 56, 64), dtype=float32)\n",
      "Tensor(\"reshape_3/Reshape:0\", shape=(?, 56, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(embedding)\n",
    "print(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Train on 8529 samples, validate on 2133 samples\n",
      "Epoch 1/10\n",
      "8529/8529 [==============================] - 7s 788us/step - loss: 0.6866 - acc: 0.5686 - val_loss: 0.6880 - val_acc: 0.6282\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62822, saving model to data/temp/weights.001-0.6282.hdf5\n",
      "Epoch 2/10\n",
      "8529/8529 [==============================] - 7s 779us/step - loss: 0.6791 - acc: 0.6386 - val_loss: 0.6835 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62822 to 0.67323, saving model to data/temp/weights.002-0.6732.hdf5\n",
      "Epoch 3/10\n",
      "8529/8529 [==============================] - 6s 722us/step - loss: 0.6680 - acc: 0.6947 - val_loss: 0.6753 - val_acc: 0.6896\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67323 to 0.68964, saving model to data/temp/weights.003-0.6896.hdf5\n",
      "Epoch 4/10\n",
      "8529/8529 [==============================] - 6s 753us/step - loss: 0.6486 - acc: 0.7485 - val_loss: 0.6593 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68964 to 0.69058, saving model to data/temp/weights.004-0.6906.hdf5\n",
      "Epoch 5/10\n",
      "8529/8529 [==============================] - 6s 706us/step - loss: 0.6142 - acc: 0.7784 - val_loss: 0.6308 - val_acc: 0.7084\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.69058 to 0.70839, saving model to data/temp/weights.005-0.7084.hdf5\n",
      "Epoch 6/10\n",
      "8529/8529 [==============================] - 6s 718us/step - loss: 0.5626 - acc: 0.8044 - val_loss: 0.5927 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.70839 to 0.72855, saving model to data/temp/weights.006-0.7286.hdf5\n",
      "Epoch 7/10\n",
      "8529/8529 [==============================] - 6s 726us/step - loss: 0.4943 - acc: 0.8339 - val_loss: 0.5544 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72855 to 0.73840, saving model to data/temp/weights.007-0.7384.hdf5\n",
      "Epoch 8/10\n",
      "8529/8529 [==============================] - 6s 710us/step - loss: 0.4239 - acc: 0.8616 - val_loss: 0.5241 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.73840 to 0.74168, saving model to data/temp/weights.008-0.7417.hdf5\n",
      "Epoch 9/10\n",
      "8529/8529 [==============================] - 6s 719us/step - loss: 0.3576 - acc: 0.8872 - val_loss: 0.5068 - val_acc: 0.7543\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74168 to 0.75434, saving model to data/temp/weights.009-0.7543.hdf5\n",
      "Epoch 10/10\n",
      "8529/8529 [==============================] - 6s 681us/step - loss: 0.3015 - acc: 0.9055 - val_loss: 0.4989 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.75434 to 0.75527, saving model to data/temp/weights.010-0.7553.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2767d390>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('data/temp/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred : 1,\t y_label : 1\n",
      "sentence : stale , futile scenario                                                    \n",
      "\n",
      "y_pred : 0,\t y_label : 0\n",
      "sentence : poetic , heartbreaking                                                     \n",
      "\n",
      "y_pred : 0,\t y_label : 0\n",
      "sentence : a modest pleasure that accomplishes its goals with ease and confidence                                             \n",
      "\n",
      "y_pred : 0,\t y_label : 1\n",
      "sentence : when a movie asks you to feel sorry for mick jagger 's sex life , it already has one strike against it                                  \n",
      "\n",
      "y_pred : 0,\t y_label : 1\n",
      "sentence : feels like six different movies fighting each other for attention                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(X_test))\n",
    "    x_test = X_test[idx].reshape(1,56)\n",
    "    y_label = y_test[idx][0]\n",
    "    y_pred = model.predict(x_test)[0][0]\n",
    "    sent = \" \".join([vocabulary_inv[x] for x in x_test[0].tolist() if x != 0])\n",
    "    print(\"y_pred : %.0f,\\t y_label : %d\\nsentence : %s\\n\" % (y_pred, y_label, sent.replace('<PAD/>', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
